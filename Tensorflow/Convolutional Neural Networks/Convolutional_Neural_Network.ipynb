{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeD800XNiRZX+fKUBpjKV4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrviSinghal28/Machine-Learning-with-Python/blob/main/Tensorflow/Convolutional%20Neural%20Networks/Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem we will consider here is classifying 10 different everyday objects. The dataset we will use is built into tensorflow and called the CIFAR Image Dataset. It contains 60,000 32x32 color images with 6000 images of each class.\n",
        "\n",
        "The labels in this dataset are the following:\n",
        "\n",
        "* Airplane\n",
        "* Automobile\n",
        "* Bird\n",
        "* Cat\n",
        "* Deer\n",
        "* Dog\n",
        "* Frog\n",
        "* Horse\n",
        "* Ship\n",
        "* Truck\n",
        "\n",
        "We'll load the dataset and have a look at some of the images below."
      ],
      "metadata": {
        "id": "YehOf1qu8RKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-82mQ_Lj9EoT",
        "outputId": "377fbe61-4d5b-487c-f44b-19c76e0b7c49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  LOAD AND SPLIT DATASET\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO4NfFz89LWv",
        "outputId": "5d18f9d0-4bde-4aa7-80ff-697cdfa9a5ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at a one image\n",
        "IMG_INDEX = 20  # change this to look at other images\n",
        "\n",
        "plt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)\n",
        "plt.xlabel(class_names[train_labels[IMG_INDEX][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "uAahpNQ39OWo",
        "outputId": "e592cd74-da03-423c-95bc-a902cd7695a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "metadata": {
        "id": "nSD5vZSw-oGI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer 1**\n",
        "\n",
        "The input shape of our data will be 32, 32, 3 and we will process 32 filters of size 3x3 over our input data. We will also apply the activation function relu to the output of each convolution operation.\n",
        "\n",
        "**Layer 2**\n",
        "\n",
        "This layer will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
        "\n",
        "**Other Layers**\n",
        "\n",
        "The next set of layers do very similar things but take as input the feature map from the previous layer. They also increase the frequency of filters from 32 to 64. We can do this as our data shrinks in spacial dimensions as it passed through the layers, meaning we can afford (computationally) to add more depth."
      ],
      "metadata": {
        "id": "BDOX_lTP-tHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()  # let's have a look at our model so far"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN7ut4ZJA2Or",
        "outputId": "bde1296d-9d92-49ed-cd68-80c85e76f79f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56,320\n",
            "Trainable params: 56,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ],
      "metadata": {
        "id": "eyeqvydKBUiZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRNJFZzZBV_D",
        "outputId": "6510a942-ff8c-42cc-bdf9-6db471ccb48c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=6, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "fHuAIrz7CChX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_KDESMPDnKh",
        "outputId": "8fd61b4e-aabe-4665-9d21-e31ee0db7960"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 3s - loss: 0.8987 - accuracy: 0.6957 - 3s/epoch - 10ms/step\n",
            "0.6956999897956848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Augmentation\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# creates a data generator object that transforms images\n",
        "datagen = ImageDataGenerator(\n",
        "rotation_range=40,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "fill_mode='nearest')\n",
        "\n",
        "# pick an image to transform\n",
        "test_img = train_images[10]\n",
        "img = image.img_to_array(test_img)  # convert image to numpy arry\n",
        "img = img.reshape((1,) + img.shape)  # reshape image\n",
        "\n",
        "i = 0\n",
        "\n",
        "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix\n",
        "    plt.figure(i)\n",
        "    plot = plt.imshow(image.img_to_array(batch[0]))\n",
        "    i += 1\n",
        "    if i > 3:  # show 4 images\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x_KlZ3UFDugL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}